### model
model: ${oc.env:WORK_DIR}/models/Qwen2.5-7B-Instruct

### method
stage: rlhf
rlhf_type: grpo
train_type: full
tuner_backend: peft
deepspeed: zero3
attn_impl: flash_attn
use_liger_kernel: true

### dataset
custom_register_path: randy/register.py
dataset: topo_sel_rlhf#20000
packing: false
max_length: 4096
dataset_num_proc: 4

### output
output_dir: ${oc.env:WORK_DIR}/train/grpo/full/Qwen2.5-7B-Instruct
logging_steps: 10
save_strategy: steps
save_steps: 100
save_total_limit: 50
report_to: tensorboard

### train
per_device_train_batch_size: 8
gradient_accumulation_steps: 1
warmup_ratio: 0.01
learning_rate: 1e-6
num_train_epochs: 1.0
lr_scheduler_type: cosine

### eval
per_device_eval_batch_size: 8
split_dataset_ratio: 0.01

### rlhf
external_plugins: randy/plugin.py
reward_funcs: format,acc_reward,gauss_reward
log_completions: true
max_completion_length: 2048
temperature: 1.0
num_generations: 8

### vllm
use_vllm: true
vllm_mode: server
vllm_server_host: 127.0.0.1
vllm_server_port: 8000
