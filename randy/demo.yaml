### model
model: ${oc.env:WORK_DIR}/models/Qwen2.5-7B-Instruct

### method
stage: sft
train_type: lora
tuner_backend: peft
lora_rank: 16
lora_alpha: 32
deepspeed: zero3
attn_impl: flash_attn
use_liger_kernel: true

### dataset
dataset_info: randy/dataset_info.json
dataset: med_mcqa
packing: false
max_length: 2048
dataset_num_proc: 16
split_dataset_ratio: 0.1

### output
output_dir: ${oc.env:WORK_DIR}/train/sft/lora/Qwen2.5-7B-Instruct
logging_steps: 10
save_strategy: steps
save_steps: 100
save_total_limit: 10
report_to: tensorboard

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 1
warmup_ratio: 0.1
learning_rate: 1e-4
num_train_epochs: 3.0
lr_scheduler_type: cosine
